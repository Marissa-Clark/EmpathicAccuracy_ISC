{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "12/26/2017  carolyn.parkinson@gmail.com\n",
    "\n",
    "After preprocessing, this script does the following for each subject:\n",
    "    1) Merges the preprocessed time series data across the 6 runs\n",
    "    2) Extracts this average preprocessed time series from each anatomical ROI\n",
    "    3) Saves the average preprocessed time series for each ROI in a dictionary\n",
    "    where the keys are the ROI labels and the values are the corresponding time\n",
    "    series.\n",
    "\"\"\"\n",
    "\n",
    "# import relevant python modules\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set path to directory containing data for all subjects\n",
    "mypath = ('/Users/mdclark/Desktop/ISCAnalysis/data/fmri/ts')\n",
    "\n",
    "# read in the list of subjects in the study whose data we'll be preprocesssing\n",
    "#list_file = '{}/subjects.json'.format(mypath)\n",
    "#with open(list_file) as data_file:\n",
    "#    subj_list = json.load(data_file)\n",
    "\n",
    "subj_list = [\"sub-289\"]\n",
    "# define a function to execute shell commands\n",
    "def sh(c):\n",
    "    '''\n",
    "    run shell commands\n",
    "    '''\n",
    "    subprocess.call(c, shell = True)\n",
    "    #print(c)\n",
    "\n",
    "# define a function to write dictionaries to files with json encoding\n",
    "def ld_writeDicts(filePath,dict):\n",
    "    f = open(filePath,'w')\n",
    "    newData = json.dumps(dict, sort_keys = True, indent = 4)\n",
    "    f.write(newData)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdclark/Desktop/ISCAnalysis/data/fmri/ts\n",
      "sub-289\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'epi5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b4a851ba20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# specify and execute an AFNI command to concatenate all runs' preprocessed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     tcat_cmd = (\"3dTcat -session {} -prefix {} \"\n\u001b[0;32m---> 31\u001b[0;31m                 \"{epi1} {epi2} {epi3} {epi4} {epi5} {epi6}\").format(nifti_dir, all_epi,  **epi_fnames)\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0msh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcat_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'epi5'"
     ]
    }
   ],
   "source": [
    "for subject in subj_list:\n",
    "\n",
    "    print(mypath)\n",
    "    print(subject)\n",
    "    # specify and enter directory containing this subject's nifti files\n",
    "    nifti_dir = \"{}/nifti/{}\".format(mypath, subject)\n",
    "    os.chdir(nifti_dir)\n",
    "\n",
    "    # also specify directory containing their SUMA files\n",
    "    suma_dir = \"{}/{}/SUMA\".format(mypath, subject)\n",
    "\n",
    "    # copy each run of preprocessed data into the subject's main nifti directory\n",
    "    epi_fnames = {}\n",
    "    for epi_num in range(1,5):\n",
    "        # specify the prefix for the preprocessed file names for this run; this is\n",
    "        # also the name of the AFNI-generated sub-directory containing these files\n",
    "        epi_prefix = 'epi{}_preprocessed'.format(epi_num)\n",
    "        # specify file name of preprocessed data within that sub-directory\n",
    "        epi_fname = '{}.cleanEPI+orig'.format(epi_prefix)\n",
    "        # create a dictionary where  keys are the run names ('epi1', 'epi2', etc.)\n",
    "        # and values are the corresponding full file names (to iterate through later)\n",
    "        epi_fnames['epi{}'.format(epi_num)] = epi_fname\n",
    "        # specify and execute an AFNI command to copy these files\n",
    "        cp_cmd = '3dcopy {}/{}/{} {}/{}.cleanEPI'.format(nifti_dir, epi_prefix, epi_fname, nifti_dir, epi_prefix)\n",
    "        sh(cp_cmd)\n",
    "\n",
    "    # specify what the file of concatenated data should be called\n",
    "    all_epi = \"{}_epi_all_preprocessed\".format(subject)\n",
    "    # specify and execute an AFNI command to concatenate all runs' preprocessed data\n",
    "    tcat_cmd = (\"3dTcat -session {} -prefix {} \"\n",
    "                \"{epi1} {epi2} {epi3} {epi4} {epi5} {epi6}\").format(nifti_dir, all_epi,  **epi_fnames)\n",
    "    sh(tcat_cmd)\n",
    "\n",
    "    # Clean up directory a bit\n",
    "    # delete redundant copies of preprocessed data for each run\n",
    "    for num, fname in epi_fnames.iteritems():\n",
    "        rm_cmd = 'rm {}/{}*'.format(nifti_dir, fname)\n",
    "        sh(rm_cmd)\n",
    "\n",
    "    # also move raw epi data into its own directory\n",
    "    raw_epi_dir = '{}/raw_epi_data'.format(nifti_dir)\n",
    "    if not os.path.exists(raw_epi_dir):\n",
    "        os.makedirs(raw_epi_dir) # create this directory if it doesn't exist already\n",
    "    mv_cmd = 'mv {}/{}_fMRI_* {}'.format(nifti_dir, subject, raw_epi_dir)\n",
    "    sh(mv_cmd)\n",
    "\n",
    "    # specify table of ROI labels from Freesurfer cortical parcellation\n",
    "    lookup_table = \"{}/aparc+aseg_rank.niml.lt\".format(suma_dir)\n",
    "\n",
    "    # create a dictionary to fill that matches ROI numbers to labels\n",
    "    roi_dict = {}\n",
    "\n",
    "    with open(lookup_table) as csvfile:\n",
    "        # skip first 4 lines (header)\n",
    "        next(csvfile)\n",
    "        next(csvfile)\n",
    "        next(csvfile)\n",
    "        next(csvfile)\n",
    "        reader = csv.DictReader(csvfile, delimiter = \" \", fieldnames = [\"roi_num\", \"roi_label\"])\n",
    "        for row in reader:\n",
    "            roi_dict[row['roi_num']] = row['roi_label']\n",
    "\n",
    "    # save as text file with json encoding\n",
    "    ld_writeDicts('{}_roi_dict_dk_atlas.json'.format(subject), roi_dict)\n",
    "\n",
    "    # Now get masks of cortical areas from Freesurfer parcellation so that we can\n",
    "    # extract the average time series from each mask.\n",
    "\n",
    "    # Get the total number of ROIs in this subject's parcellation file; this is\n",
    "    # 1 less than the length of the lookup table due to the file footer. The\n",
    "    # length of different subjects' lookup tables may vary a bit, as some\n",
    "    # subjects have a 5th ventricle in the FreeSurfer subcortical segmentation;\n",
    "    # see http://surfer.nmr.mgh.harvard.edu/fswiki/SubcorticalSegmentation/\n",
    "    all_roi_num = range(1, (len(roi_dict)-1))\n",
    "\n",
    "    # get Freesurfer parcellation file that's been aligned to our anatomical scan\n",
    "    aparc = \"{}/aparc+aseg_rank_Alnd_Exp\".format(nifti_dir)\n",
    "\n",
    "    # resample parcellation file to functional resolution\n",
    "    resamp_cmd = (\"3dresample -master {}+orig -rmode NN \"\n",
    "                  \"-input {}+orig -prefix {}_3mm\").format(all_epi, aparc, aparc)\n",
    "    sh(resamp_cmd)\n",
    "\n",
    "    # dictionary to store preprocessed time series indexed by ROI\n",
    "    subj_ts_dict = {}\n",
    "\n",
    "    for roi_id in all_roi_num:\n",
    "        # extract ROI label form ROI dictionary\n",
    "        label_roi = roi_dict[str(roi_id)]\n",
    "\n",
    "        # Extract the average time series across voxels that ROI\n",
    "        avg_cmd = (\"\"\"3dmaskave -quiet -mask {}_3mm+orig\"<{}..{}>\" {}+orig > avg_{}.1D\"\"\").format(aparc, roi_id, roi_id, all_epi, label_roi)\n",
    "        sh(avg_cmd)\n",
    "\n",
    "        # Then read in the 1D file generated by the above command (average time\n",
    "        # series across voxels within a given ROI) and add it to a dictionary\n",
    "        # indexed by ROI label\n",
    "        this_vec = \"avg_{}.1D\".format(label_roi)\n",
    "        subj_ts_dict[label_roi] = np.loadtxt(this_vec).tolist()\n",
    "\n",
    "    # save dictionary to a text file with json encoding\n",
    "    ld_writeDicts('{}_roi_dk_atlas_ts_dict.json'.format(subject), subj_ts_dict)\n",
    "\n",
    "    # move the individual ROI-wise time series 1D files to their own sub-directory\n",
    "    sub_dir = \"{}/avg\".format(nifti_dir)\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir) # create qvecs if it doesn't exist\n",
    "    mv_sub_cmd = \"mv avg*1D avg/\"\n",
    "    sh(mv_sub_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
